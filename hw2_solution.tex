\documentclass{article}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{geometry}
\geometry{left=2.0cm,right=2.0cm,top=2.5cm,bottom=2.5cm}
\newtheoremstyle{break}%
    {}{}%
    {\itshape}{}%
    {\bfseries}{}% % Note that final punctuation is omitted.
    {\newline}{}
\theoremstyle{break}
\newtheorem*{definition}{Definition}
\newtheorem*{proof_break}{Proof.}
\lstset{
    numbers=left, 
    numberstyle= \tiny, 
    keywordstyle= \color{ blue!70},
    commentstyle= \color{red!50!green!50!blue!50}, 
    frame=shadowbox, % 阴影效果
    rulesepcolor= \color{ red!20!green!20!blue!20} ,
    escapeinside=``, % 英文分号中可写入中文
    xleftmargin=2em,xrightmargin=2em, aboveskip=1em,
    framexleftmargin=2em
} 

\begin{document}
    \title{HW2}
    \author{Sitian Qian 1600011388}
    \maketitle

    \section{Problem 1}
    \subsection{Subproblem 1}
    \begin{proof_break}\par
        Firstly we can write $\hat{\sigma}_a^2$ and $\hat{\sigma}_b^2$ as following:
        $$\left[\begin{array}{c}{\hat{\sigma}_{a}^{2}} \\ {\hat{\sigma}_{b}^{2}}\end{array}\right]=\frac{1}{nq} \sum_{k=1}^{n}\left[\begin{array}{c}{\sum_{j=1}^{q}(\varepsilon_{q(k-1)+j}^2)} \\ {(\sum_{j=1}^{q}\varepsilon_{q(k-1)+j})^{2}}\end{array}\right]$$
        Since $\{\varepsilon_{qk}\}$ is an iid sequence, $\{\sum_{j=1}^{q}(\varepsilon_{q(k-1)+j}^2)\}$,$\{(\sum_{j=1}^{q}\varepsilon_{q(k-1)+j})^{2}\}$ are also iid sequences. Which enables us to use central limit theorem to obtain the
        asymptotic distribution.\newline
        The expectation is easy to obtain: 
        $$\mathbb{E}\left[\begin{array}{c}{\sum_{j=1}^{q}(\varepsilon_{q(k-1)+j}^2)} \\ {(\sum_{j=1}^{q}\varepsilon_{q(k-1)+j})^{2}}\end{array}\right]=\left[\begin{array}{l}{q \sigma^{2}} \\ {q \sigma^{2}}\end{array}\right]$$
        The variance can be obtained as below: 
        \begin{align*}
            &Var(\sum_{j=1}^{q}(\varepsilon_{q(k-1)+j}^2))\\
            =&\sum_{j=1}^q(Var(\varepsilon^2_{q(k-1)+j}))\\
            =&q(A-1)\sigma^4
        \end{align*}
        (Here I use $A$ to represent $\mathbb{E}{x}^4$, x is subject to standard normal distribution, in fact $A=3$;)
        \begin{align*}
            &Var((\sum_{j=1}^{q}\varepsilon_{q(k-1)+j})^2)\\
            =&qA\sigma^4+6*C_q^2\sigma^4-q^2*\sigma^4\\
            =&(Aq+2q^2-3q)\sigma^4
        \end{align*}
        and
        \begin{align*}
            &Cov((\sum_{j=1}^{q}\varepsilon_{q(k-1)+j})^2,\sum_{j=1}^{q}(\varepsilon_{q(k-1)+j}^2))\\
            =&Var(\sum_{j=1}^{q}(\varepsilon_{q(k-1)+j}^2))+Cov(\sum_{i=1}^{q}\sum_{j\neq{i},j=1}^{q}(\varepsilon_{q(k-1)+i}\varepsilon_{q(k-1)+j}),\sum_{j=1}^{q}(\varepsilon_{q(k-1)+j}^2))\\
            =&q(A-1)\sigma^4
        \end{align*}
        Finally we get:
        \begin{align*}
        Var\left(\left[\begin{array}{c}{\sum_{j=1}^{q}(\varepsilon_{q(k-1)+j}^2)} \\ {(\sum_{j=1}^{q}\varepsilon_{q(k-1)+j})^2}\end{array}\right]\right)=\left[\begin{array}{cc}{2q\sigma^4} & {2q\sigma^4}\\ {2q\sigma^{4}} & {2q^2\sigma^{4}}\end{array}\right]
        \end{align*}
        Therefore, using CLT,
            $$\sqrt{n}\left(\left[\begin{array}{c}{\hat{\sigma}_{a}^{2}} \\ {\hat{\sigma}_{b}^{2}}\end{array}\right]-\left[\begin{array}{c}{\sigma^{2}} \\ {\sigma^{2}}\end{array}\right]\right) \rightarrow_{d} \mathbb{N}\left(0,\left[\begin{array}{cc}{\frac{2\sigma^{4}}{q}} & {\frac{2\sigma^4}{q}} \\ {\frac{2\sigma^{4}}{q}} & {2\sigma^4}\end{array}\right]\right)$$
        With the Delta method, representing $J_r=\frac{\hat{\sigma}_b^2}{\hat{\sigma}_a^2}-1$ by $J_{r}=h(\hat{\sigma}_{a}^{2}, \hat{\sigma}_{b}^{2})$,we have: 
        $$\sqrt{n}(h(\hat{\beta})-h(\beta)) \rightarrow_{d} \mathbb{N}\left(0, \nabla h^{T}(\beta) D \nabla h(\beta)\right.$$
        where $\hat{\beta}=(\hat{\sigma}_{a}^{2}, \hat{\sigma}_{b}^{2})^{T}$, $\beta=\vec{0}$, and $D=\left[\begin{array}{cc}{\frac{2\sigma^{4}}{q}} & {\frac{2\sigma^4}{q}} \\ {\frac{2\sigma^{4}}{q}} & {2\sigma^4}\end{array}\right]$,
        \newline
        Namely, the distribution of $J_r$ is,
        $$\sqrt{n}\left(J_{r}-0\right) \rightarrow_{d} \mathbb{N}\left(0,\left[\begin{array}{cc}{-\frac{1}{\sigma^{2}}} & {\frac{1}{\sigma^{2}}}\end{array}\right]\left[\begin{array}{cc}{\frac{2\sigma^{4}}{q}} & {\frac{2\sigma^4}{q}} \\ {\frac{2\sigma^{4}}{q}} & {2\sigma^4}\end{array}\right]\left[\begin{array}{c}{-\frac{1}{\sigma^{2}}} \\ {\frac{1}{\sigma^{2}}}\end{array}\right]\right)=\mathbb{N}(0,\frac{2(q-1)}{q})$$
        $\rm{Q.E.D}$
    \end{proof_break}
    \section{Problem 2}
    The code I write to conduct VR test is shown below:
    \begin{lstlisting}[language={python}]
import numpy as np
import scipy.stats as st

def J_r_calc(data,q):
    '''
    Calculation of J_r statistic
    
    Parameters:
    
    data: 1D array, 
    time series data of a certain stock (difference between each time point)
    
    q: int, parameter of the VR test
    
    Return:
    
    return 1: float, value of J_r statistic
    
    return 2: int, n of the VR test, namely Size of data devided by q
    
    '''
    # modify the length of data to nq
    if len(data)%q != 0:
        data = data[len(data)%q:]
    
    n = len(data)//q
    
    sigma_a_sq=np.power(data,2).sum()
    # print(q,n,len(data))
    data_q = data.reshape(q,n)
    sigma_b_sq=np.power(data_q.sum(axis=0),2).sum()
    
    return sigma_b_sq/sigma_a_sq - 1 ,n

def VR_test(data,q):
    '''
    Operate the VR test to the time series data
    
    Parameters:
    
    data: 1D array, 
    time series data of a certain stock (difference between each time point)
    
    q: int, parameter of the VR test
    
    Return:
    
    return 1: float, p-value of the VR test
    
    '''
    
    J_r,n=J_r_calc(data,q)
    
    sigma=np.sqrt(2*(q-1)/q)
    
    # print(sigma,J_r)
    
    return (1-st.norm.cdf(np.sqrt(n)*abs(J_r)/sigma))*2
    \end{lstlisting}
    I choose the IBM's stock, using its monthly return data from 1970/01 to 2018/03 as input to conduct VR test, here I neglect the data pre-processing part,and the whole jupyter notebook is displayed in the appendix.
    \newline
    Then conduct VR test for $q=2,3,6,12$:
    \begin{lstlisting}[language={python}]
for q in [2,3,6,12]:
print("q={:d},p-value={:.4f}".format(q,VR_test(IBM_return,q)))
# the variable IBM_return is the monthly return of IBM
    \end{lstlisting}
    The output is shown below:
    \begin{lstlisting}
q=2,p-value=0.9516
q=3,p-value=0.1601
q=6,p-value=0.6176
q=12,p-value=0.3126
    \end{lstlisting}
    I choose the precession to be 4 digits after dot for simplicity.
    \section{Problem 3}
    It's obvious to find that since $S^*$ is the max among those $(|\sqrt{\frac{nq}{2(q-1)}}J_r(q)|)$s, and every $\sqrt{\frac{nq}{2(q-1)}}J_r(q)$ is subject to a standard normal distribution, so if $S^*$ is bigger than the $(1-\alpha^+/2)$-th percentile of the standard normal distribution, it is equal to that the smallest p-value from VR test to different $q$ is smaller than $\alpha^+$. 
    \newline
    So I can write code like this:
    \begin{lstlisting}[language={python}]
alpha_plus=1-(1-0.05)**(1/4) #alpha=0.05
p_values=[VR_test(IBM_return,q) for q in [2,3,6,12]]
S_star_p=min(p_values)
if(S_star_p<alpha_plus):
    print('Reject')
else:
    print("Not Reject")     
    \end{lstlisting}
    The output is "Not Reject". Namely, we can not reject the null hypothesis with $\alpha=0.05$.
\end{document}